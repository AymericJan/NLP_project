{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Project",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFUClYUKhLK7",
        "outputId": "925f7f0c-ca9b-443d-e185-d036d546848c"
      },
      "source": [
        "import re\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "from sklearn.svm import LinearSVC\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, Input\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6gZ1NPXhd9y"
      },
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "test_labels = pd.read_csv('test_labels.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4X3mky2hd6X"
      },
      "source": [
        "ids_to_discard = test_labels[test_labels['toxic'] == -1]['id']\n",
        "test = test[~test['id'].isin(ids_to_discard)]\n",
        "test_labels = test_labels[~test_labels['id'].isin(ids_to_discard)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L75fm1mhd3O"
      },
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"what's\", \"what is \", text)      ### conversion of contraction words to expanded words\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"can not \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
        "    text = re.sub('\\W', ' ', text)                                                 ### removing non-word characters\n",
        "\n",
        "    text = re.sub(r'fck', 'fuck', text)\n",
        "    text = re.sub(r'a$$', 'ass', text)\n",
        "    text = re.sub(r'@', 'at', text)\n",
        "    text = re.sub(r'wikipedia:[^\\s]+',' ', text)\n",
        "    text = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n",
        "    text = re.sub('(utc)', ' ' ,text)\n",
        "    text = re.sub(' u ', ' you ' ,text)\n",
        "\n",
        "\n",
        "    text = re.sub('[^A-Za-z\\' ]+', '',text)                                        ### removing all non-letter values(Except single quotes)\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "\n",
        "    text = text.strip(' ')\n",
        "    text = ' '.join([word for word in text.split() if word not in (stop_words)])    ### Stopwords removal\n",
        "    return text\n",
        "\n",
        "train[\"comment_text\"] = train[\"comment_text\"].apply(clean_text)\n",
        "test[\"comment_text\"] = test[\"comment_text\"].apply(clean_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtWvxFlzhdv2"
      },
      "source": [
        "train_data = train[\"comment_text\"]\n",
        "test_data = test[\"comment_text\"]\n",
        "# train_label=train[['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']]\n",
        "train_label=train['toxic']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKIuNIhLhdsY"
      },
      "source": [
        "### Creating corpus of words and coverting it into integer and then susbstituing it in sentences - prepare tokenizer\n",
        "tokenizer = Tokenizer(num_words = 40000) #40000 words are used here\n",
        "tokenizer.fit_on_texts(train_data)\n",
        "\n",
        "#convert each text into array of integers with help of tokenizer.\n",
        "train_final = tokenizer.texts_to_sequences(train_data)\n",
        "test_final = tokenizer.texts_to_sequences(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7Fdl7Dnl-k2",
        "outputId": "b7166943-b119-4c83-8870-fd104cc2ec17"
      },
      "source": [
        "train_padded =pad_sequences(train_final, maxlen=150)\n",
        "test_padded =pad_sequences(test_final, maxlen=150)\n",
        "print(\"Shape of training data\",train_padded.shape)\n",
        "print(\"Shape of testing data\",test_padded.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of training data (159571, 150)\n",
            "Shape of testing data (63978, 150)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkDVt8h8oTMP"
      },
      "source": [
        "#Vectorize words using word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo9Tq-dgmwiY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kkBbqMKmweK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GWQL9hTmwc-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LYqSP0xmvXu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knOKi3oWmtGJ"
      },
      "source": [
        "# Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oD2GgkhihdoX",
        "outputId": "e6dc5a5a-6d81-4592-edb5-687fa1df052c"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(40000, 128))\n",
        "model.add(LSTM(units = 64, dropout = 0.2,return_sequences=True))\n",
        "model.add(LSTM(units = 64, dropout = 0.2))\n",
        "model.add(Dense(units = 1, activation = 'sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, None, 128)         5120000   \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, None, 64)          49408     \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, 64)                33024     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 5,202,497\n",
            "Trainable params: 5,202,497\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oudlqihkt0p",
        "outputId": "6fe210e0-ee77-4c7b-e608-3d409f6ee6b5"
      },
      "source": [
        "model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"AUC\", \"accuracy\"])\n",
        "x_train, x_val, y_train, y_val = train_test_split(train_padded, train_label, shuffle = True, random_state = 123)\n",
        "model.fit(x_train, y_train, batch_size = 32, epochs = 1, validation_data = (x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3740/3740 [==============================] - 912s 243ms/step - loss: 0.1347 - auc: 0.9472 - accuracy: 0.9531 - val_loss: 0.0991 - val_auc: 0.9740 - val_accuracy: 0.9644\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdce6cadd90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP_r6IMAktxu"
      },
      "source": [
        "### Prediction for test data\n",
        "predict = model.predict(test_padded)\n",
        "print(\"Predicted values are\",predict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZfnlpYQktu2"
      },
      "source": [
        "target_cols = np.array(['toxic','severe_toxic','obscene', 'threat','insult', 'identity_hate'])\n",
        "final_predict_test = pd.concat([pd.DataFrame(predict, columns=target_cols)], 1)\n",
        "t1 = test['id']\n",
        "final_predict_test = pd.concat([t1,final_predict_test],1)\n",
        "final_predict_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}